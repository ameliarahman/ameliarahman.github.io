---
layout: post
title: "Introduction to Pgai"
description: "What is pgai, how to use it and how it can automate AI embedding"
date: 2025-03-18 10:00:00
comments: true
keywords: "Database, Postgres, Query, pgai, llm, OpenAI, RAG, machine learning, ml"
category: [Tech]
tags:
- database, postgresql, sql, ai, pgai, llm, openai, rag, machine learning, ml
---


The slogan **"Postgres for everything"** is proving more true every day. The Timescale - a company behind TimescaleDB, the open-source time-series database built on PostgreSQL - has developed **pgai**, a powerful open-source extension that enables developer to work with machine learning model without the hassle of setting up multiple tools.

By using **pgai**, we don't need to worry if we are not an AI/ML expert. As long as we understand the SQL syntax, this extension can help us to automatically create and synchronize vector embeddings for our data, support Retrieval Augmented Generation (RAG), semantic search, and even allow direct calls to LLMs like OpenAI, Ollama, Cohere, etc by just using simple SQL commands.

The documentation is very easy to follow: <a href="https://github.com/timescale/pgai" target="_blank"> Pgai Documentation</a> and here's what I’ve accomplished while exploring **pgai**.

## Getting Started 
First thing first, here is my docker compose file to get started:
<script src="https://gist.github.com/ameliarahman/132c3d6f246f6e6c2fc09728af9cbf6e.js"></script>
Make sure to set the ***OPENAI_API_KEY*** in your .env file. 

I explore the pgai vectorizer with OpenAI following this guide: <a href="https://github.com/timescale/pgai/blob/main/docs/vectorizer/quick-start-openai.md" target="_blank"> Quick Start - OpenAI </a>. If you’re interested in trying other tools like Ollama or Voyage, there are quick-start guides available as well!

## Enable Extension **pgai**
If we check the extension by using query:
```sql
SELECT * FROM pg_extension;
```

There's still no **ai** extension yet, so we need to create it first:

```sql
CREATE EXTENSION IF NOT EXISTS ai CASCADE;
```
By creating this extension, this also creates new schema named **ai** in our database.
![](../assets/img/pgai/schema.png)

## Create Table and Seed Data
There are two ways to complete this step:

#### 1. Create table and seed manually

<script src="https://gist.github.com/ameliarahman/f808f72b1b3c9c97aa0dc7f365b7174b.js"></script>

_Note: I ask Chat GPT to give me 10 random data_


#### 2. Use `ai.load_dataset` command

Basically, this command is to load the data from the Hugging Face, so we need the active internet connection to download the datasets from <a href="https://huggingface.co/datasets/" target="_blank">here</a>.

You can choose how to load your dataset following <a href="https://github.com/timescale/pgai/blob/main/docs/utils/load_dataset_from_huggingface.md" target="_blank">this documentation</a>.

Here's just 1 example how I load the small subset of `CodeKapital/CookingRecipes` datasets:

```sql
SELECT ai.load_dataset('CodeKapital/CookingRecipes', batch_size => 100, max_batches => 1);
```

This command creates new table named `cookingrecipes` and seed 100 data from the Hugging Face.
![](../assets/img/pgai/table_1.png)
![](../assets/img/pgai/column_and_data.png)

## Create Vectorizer

To create a vectorizer, let's run the SQL command below:

```sql
SELECT ai.create_vectorizer(
   'blogs'::regclass,
   destination => 'blogs_contents_embeddings',
   embedding => ai.embedding_openai('text-embedding-3-small', 768, api_key_name=>'OPENAI_API_KEY'),
   chunking => ai.chunking_recursive_character_text_splitter('content')
);
```
- 'blogs'::regclass, refers to the table name `blogs` and regclass is to cast the string `blogs` into a PostgreSQL object identifier (OID). <a href="https://www.postgresql.org/docs/current/datatype-oid.html" target="_blank">Read more about Object Identifier Type</a>.
- destination => 'blogs_contents_embeddings', means the destination to store the embeddings will be in a view named `blogs_contents_embeddings`
- embedding => ai.embedding_openai('text-embedding-3-small', 768), means this use OpenAI's `text-embedding-3-small` model to generate `768-dimensional vector embeddings`.
- chunking => ai.chunking_recursive_character_text_splitter('content'), aims to break large text in `content` column into smaller chunks before embedding.

We can check the vectorizer status from the view named `vectorizer_status` in `ai` schema:
```sql
SELECT * from ai.vectorizer_status;
```
![](../assets/img/pgai/vectorizer_status.png)

Stated from pgai documentation: 
`All the embeddings have been created when the pending_items column is 0. This may take a few minutes as the model is running locally and not on a GPU.`

We can also check from the vectorizer worker logs whether the processing is already finished or not:

```
docker logs -f <container_name_or_id>
```
![](../assets/img/pgai/container_vectorizer.png)

Every 5 minutes, the worker will check the vectorizer for pending tasks and begin processing any queued items.

Once processing is complete, the embeddings will be inserted into the table:
![](../assets/img/pgai/embedding_1.png)

Let's do the same thing for `cookingreceips` table:
```sql
SELECT ai.create_vectorizer(
   'cookingrecipes'::regclass,
   destination => 'cookingrecipes_directions_embeddings',
   embedding => ai.embedding_openai('text-embedding-3-small', 768, api_key_name=>'OPENAI_API_KEY'),
   chunking => ai.chunking_recursive_character_text_splitter('directions')
);
```

_Note: Before executing the command above, I rename the column unnamed to id and add constraint primary key on it_

Here is the vectorizer status:
![](../assets/img/pgai/vectorizer_status_2.png)

And here is the embedding result:
![](../assets/img/pgai/embedding_2.png)


All the process above can be seen in the image below:
![](../assets/img/pgai/pgai_architecture.png)
(source: 
<a href="https://www.linkedin.com/feed/update/urn:li:activity:7257016489830961153/" target="_blank">Pgai Vectorizer System Overview</a>)

Our source data tables are stored in the `public` schema. Whenever new data is added, it triggers a queue table in the `ai` schema. The worker periodically checks for pending tasks, and if any exist, it processes them by generating vector embeddings based on the specified syntax. Once the embeddings are generated, they are stored in the designated embedding table.

This ensures that whenever new data is inserted into `blogs` or `cookingrecipes` table, the vector embeddings are always automatically created and synchronized in real time.

## Semantic Search
Now, let's try a semantic search by searching data related to `psychological health` in the `blogs`. But before doing that, let's make sure that there is no `psychological health` words in `blogs` table:
![](../assets/img/pgai/check_1.png)

And here is the full query to search the embeddings: 

```sql
SELECT
    chunk,
    embedding <=>  ai.openai_embed('text-embedding-3-small', 'psychological health', dimensions=>768) as distance
FROM blogs_contents_embeddings
ORDER BY distance;
```

Here is the result:
![](../assets/img/pgai/result_1.png)

The larger the distance, the less relevant the data.

Another example for `cookingrecipes`. Let's try to search `food for winter`:

```sql
SELECT
    chunk,
    embedding <=>  ai.openai_embed('text-embedding-3-small', 'food for winter', dimensions=>768) as distance
FROM cookingrecipes_directions_embeddings cde
ORDER BY distance;
```
Here is the image of partial result:
![](../assets/img/pgai/result_2.png)

The function ai.openai_embed is to call `text-embedding-3-small` model. To read more about the pgai's model calling feature, you can refer to: https://github.com/timescale/pgai#model-calling
### RAG Mini Application

The documentation also stated: _Semantic search is a powerful feature in its own right, but it is also a key component of Retrieval Augmented Generation (RAG)_

We can leverage the SQL command above to build a simple RAG (Retrieval-Augmented Generation) mini-application. And I try to integrate it into a Go CLI:

So here is the full code:

1. Let's define the CLI syntax for receiving user input in our code:
<script src="https://gist.github.com/ameliarahman/4077659c285266d8050b3bd5ca696dfb.js"></script>

2. Create function to retrieve the data using the SQL command above:
<script src="https://gist.github.com/ameliarahman/bbeb4790e92318815da3b3c98e765a91.js"></script>

And here is the result:

Question: _It's very hot in my
place right now, and I want to cook something light and refreshing. How do I make a perfect summer salad?_
![](../assets/img/pgai/rag_1.png)

Question: _I just got fresh chili peppers from the market, and I really love spicy food. How do I cook a fiery, flavorful dish?_
![](../assets/img/pgai/rag_2.png)

That's it.
![](../assets/img/pgai/rag_3.png)







